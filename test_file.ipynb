{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing functions from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the object to call LLMs from groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planet with the most moons in our solar system is Jupiter. As of 2023, Jupiter has a total of 92 confirmed moons. The four largest moons of Jupiter, known as the Galilean moons, are Io, Europa, Ganymede, and Callisto. These four moons were discovered by Galileo Galilei in 1610 and are some of the largest moons in the solar system.\n",
      "\n",
      "Here's a list of the top 5 planets with the most moons in our solar system:\n",
      "\n",
      "1. Jupiter - 92 confirmed moons\n",
      "2. Saturn - 83 confirmed moons\n",
      "3. Uranus - 27 confirmed moons\n",
      "4. Neptune - 14 confirmed moons\n",
      "5. Mars - 2 confirmed moons (Phobos and Deimos)\n",
      "\n",
      "It's worth noting that the number of moons can change as new discoveries are made, and some sources may group smaller objects like moonlets or ring particles differently. However, as of now, Jupiter has the most moons in our solar system.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what planet in the solar system has more moons?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pdf files and trasnforming to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "current_wd = os.getcwd()\n",
    "data_path = os.path.join(current_wd, 'data')\n",
    "raw_documents = os.listdir(data_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file_name in raw_documents:\n",
    "    with pdfplumber.open(os.path.join(data_path, file_name)) as pdf:\n",
    "        text = ''\n",
    "        for p in pdf.pages:\n",
    "            text += p.extract_text().replace('\\n', ' ')\n",
    "        documents.append({\"id\": file_name, \"text\": text})\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk the text from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n"
     ]
    }
   ],
   "source": [
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Load Pretrained Model and Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Example Text\n",
    "# text = \"Hello, how are you?\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# # Run the input through the model\n",
    "# with torch.inference_mode():  # Disable gradient calculation for efficiency\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Extract the hidden states (last layer embeddings)\n",
    "# last_hidden_states = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# cls_embedding = last_hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "# print(cls_embedding.shape)  # Output: torch.Size([1, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "    def __call__(self, input_texts):\n",
    "        if isinstance(input_texts, str):\n",
    "            input_texts = [input_texts]\n",
    "        embeddings = self.model.encode(input_texts, convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "custom_embeddings = MyEmbeddingFunction()\n",
    "\n",
    "\n",
    "chroma_client = PersistentClient(path=\"chroma_persistent_storage\")\n",
    "test_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"ghg_collection\",\n",
    "    embedding_function=custom_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks stored in ChromaDB: 2126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in chunked_documents:\n",
    "    test_collection.add(\n",
    "        ids=[chunk[\"id\"]],\n",
    "        embeddings=custom_embeddings([chunk[\"text\"]]),\n",
    "        metadatas=[{\"source\": chunk[\"id\"]}],\n",
    "        documents=[chunk[\"text\"]]\n",
    "    )\n",
    "\n",
    "print(f\"Total chunks stored in ChromaDB: {test_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Query Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(question, n_results=2):\n",
    "\n",
    "    query_embedding = custom_embeddings([question])\n",
    "\n",
    "    results = test_collection.query(query_embeddings=query_embedding, n_results=n_results)\n",
    "\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    print(\"==== Returning relevant chunks ====\")\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are a digital consultant specializing in Australia's evolving greenhouse gas (GHG) emission regulations. \"\n",
    "        \"Your task is to help companies navigate the complexities of compliance, accurate emission calculations, and industry-specific scope definitions. \"\n",
    "        \"Use the following context to provide tailored, concise, and accurate guidance. Ensure the response is practical, actionable, and aligned with the most recent regulatory updates. \"\n",
    "        \"If the answer is not available or unclear, state that you do not know. \"\n",
    "        \"Use five sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
    "    )\n",
    "\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Returning relevant chunks ====\n",
      "To calculate Scope 3 emissions, your company should refer to the relevant industry segment description in Section 2 of the API Compendium and identify the operations and sources that need to be assessed. Section 3 provides guidance on equipment classification and inventory accuracy. For actual calculation methodologies, your company may need to look beyond the provided context, as it mainly focuses on direct emissions (Scope 1) from combustion devices, waste gas disposal, process and operational venting, and fugitive emission sources. I am not aware of specific guidance for Scope 3 emissions in the given context. You may need to consult additional resources or regulatory updates for accurate Scope 3 emission calculations.\n"
     ]
    }
   ],
   "source": [
    "question = \"How should my company calculate emissions for Scope 3?\"\n",
    "relevant_chunks = query_documents(question)\n",
    "response = generate_response(question, relevant_chunks)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
