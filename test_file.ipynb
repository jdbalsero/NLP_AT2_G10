{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_assignment_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DataPreprocessor...\n",
      "The function _initialize_collection is running\n",
      "The function read_pdf_files is running\n",
      "Number of documents processed: 2\n",
      "\n",
      "Processing document: C2024C00572.pdf\n",
      "The function split_text is running\n",
      "Number of chunks created: 297\n",
      "\n",
      "Sample chunk (first 200 characters):\n",
      "National Greenhouse and Energy Reporting Act 2007 No. 175, 2007 Compilation No. 26 Compilation date: 14 October 2024 Includes amendments: Act No. 39, 2024 Prepared by the Office of Parliamentary Couns...\n"
     ]
    }
   ],
   "source": [
    "from nlp.data_preprocessing import DataPreprocessor\n",
    "\n",
    "# Initialize the preprocessor with default paths\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Test reading PDF files\n",
    "documents = preprocessor.read_pdf_files()\n",
    "print(f\"Number of documents processed: {len(documents)}\")\n",
    "\n",
    "# Test text splitting on the first document if any were found\n",
    "if documents:\n",
    "    first_doc = documents[0]\n",
    "    print(f\"\\nProcessing document: {first_doc['id']}\")\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = preprocessor.split_text(first_doc['text'])\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    \n",
    "    # Print first chunk as sample\n",
    "    if chunks:\n",
    "        print(\"\\nSample chunk (first 200 characters):\")\n",
    "        print(chunks[0][:200] + \"...\")\n",
    "else:\n",
    "    print(\"No PDF documents found in the data directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatBot tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing functions from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the object to call LLMs from groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planet in our solar system with the most moons is Jupiter. Jupiter has a total of 92 confirmed moons, with many more smaller moonlets and moon fragments orbiting the planet. Some of the most notable moons of Jupiter include Io, Europa, Ganymede, and Callisto, which are known as the Galilean moons because they were discovered by Galileo Galilei in 1610.\n",
      "\n",
      "Here's a rough breakdown of the number of moons for each planet in our solar system:\n",
      "\n",
      "* Jupiter: 92 confirmed moons\n",
      "* Saturn: 83 confirmed moons\n",
      "* Uranus: 27 confirmed moons\n",
      "* Neptune: 14 confirmed moons\n",
      "* Mars: 2 confirmed moons (Phobos and Deimos)\n",
      "* Earth: 1 confirmed moon (The Moon)\n",
      "* Venus: 0 confirmed moons\n",
      "* Mercury: 0 confirmed moons\n",
      "\n",
      "It's worth noting that the number of moons for each planet can vary slightly depending on the source, as new moons are still being discovered and some smaller moonlets may not be officially confirmed.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    #api_key=\"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what planet in the solar system has more moons?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pdf files and trasnforming to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/user/Documents/GitHub/UTS/Applied_NLP/NLP_AT2_G10/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m current_wd = os.getcwd()\n\u001b[32m      3\u001b[39m data_path = os.path.join(current_wd, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m raw_documents = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m documents = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/user/Documents/GitHub/UTS/Applied_NLP/NLP_AT2_G10/data'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "current_wd = os.getcwd()\n",
    "data_path = os.path.join(current_wd, 'data')\n",
    "raw_documents = os.listdir(data_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file_name in raw_documents:\n",
    "    with pdfplumber.open(os.path.join(data_path, file_name)) as pdf:\n",
    "        text = ''\n",
    "        for p in pdf.pages:\n",
    "            text += p.extract_text().replace('\\n', ' ')\n",
    "        documents.append({\"id\": file_name, \"text\": text})\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk the text from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n"
     ]
    }
   ],
   "source": [
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Load Pretrained Model and Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Example Text\n",
    "# text = \"Hello, how are you?\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# # Run the input through the model\n",
    "# with torch.inference_mode():  # Disable gradient calculation for efficiency\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Extract the hidden states (last layer embeddings)\n",
    "# last_hidden_states = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# cls_embedding = last_hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "# print(cls_embedding.shape)  # Output: torch.Size([1, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_assignment_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "    def __call__(self, input_texts):\n",
    "        if isinstance(input_texts, str):\n",
    "            input_texts = [input_texts]\n",
    "        embeddings = self.model.encode(input_texts, convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "custom_embeddings = MyEmbeddingFunction()\n",
    "\n",
    "\n",
    "chroma_client = PersistentClient(path=\"chroma_persistent_storage\")\n",
    "test_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"ghg_collection\",\n",
    "    embedding_function=custom_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks stored in ChromaDB: 2126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in chunked_documents:\n",
    "    test_collection.add(\n",
    "        ids=[chunk[\"id\"]],\n",
    "        embeddings=custom_embeddings([chunk[\"text\"]]),\n",
    "        metadatas=[{\"source\": chunk[\"id\"]}],\n",
    "        documents=[chunk[\"text\"]]\n",
    "    )\n",
    "\n",
    "print(f\"Total chunks stored in ChromaDB: {test_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Query Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(question, n_results=2):\n",
    "\n",
    "    query_embedding = custom_embeddings([question])\n",
    "\n",
    "    results = test_collection.query(query_embeddings=query_embedding, n_results=n_results)\n",
    "\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    print(\"==== Returning relevant chunks ====\")\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are a digital consultant specializing in Australia's evolving greenhouse gas (GHG) emission regulations. \"\n",
    "        \"Your task is to help companies navigate the complexities of compliance, accurate emission calculations, and industry-specific scope definitions. \"\n",
    "        \"Use the following context to provide tailored, concise, and accurate guidance. Ensure the response is practical, actionable, and aligned with the most recent regulatory updates. \"\n",
    "        \"If the answer is not available or unclear, state that you do not know. \"\n",
    "        \"Use five sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
    "    )\n",
    "\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Returning relevant chunks ====\n",
      "To calculate Scope 3 emissions, your company should follow the Australian Government's National Greenhouse and Energy Reporting (NGER) scheme and the GHG Protocol Corporate Standard. Scope 3 emissions include indirect emissions from sources not owned or controlled by your company, such as supply chain, transportation, and employee commuting. You can use the GHG Protocol's Scope 3 calculation guidance and tools to estimate these emissions. The Australian Government also provides resources and tools to support Scope 3 emissions calculation, including the NGER Scope 3 Emissions Estimation Tool. It is recommended to consult with a specialist or the relevant Australian authorities to ensure compliance with the latest regulatory requirements.\n"
     ]
    }
   ],
   "source": [
    "question = \"How should my company calculate emissions for Scope 3?\"\n",
    "relevant_chunks = query_documents(question)\n",
    "response = generate_response(question, relevant_chunks)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
