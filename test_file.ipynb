{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_assignment_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DataPreprocessor...\n",
      "The function _initialize_collection is running\n",
      "The function read_pdf_files is running\n",
      "Number of documents processed: 2\n",
      "\n",
      "Processing document: C2024C00572.pdf\n",
      "The function split_text is running\n",
      "Number of chunks created: 297\n",
      "\n",
      "Sample chunk (first 200 characters):\n",
      "National Greenhouse and Energy Reporting Act 2007 No. 175, 2007 Compilation No. 26 Compilation date: 14 October 2024 Includes amendments: Act No. 39, 2024 Prepared by the Office of Parliamentary Couns...\n"
     ]
    }
   ],
   "source": [
    "from nlp.data_preprocessing import DataPreprocessor\n",
    "\n",
    "# Initialize the preprocessor with default paths\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Test reading PDF files\n",
    "documents = preprocessor.read_pdf_files()\n",
    "print(f\"Number of documents processed: {len(documents)}\")\n",
    "\n",
    "# Test text splitting on the first document if any were found\n",
    "if documents:\n",
    "    first_doc = documents[0]\n",
    "    print(f\"\\nProcessing document: {first_doc['id']}\")\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = preprocessor.split_text(first_doc['text'])\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    \n",
    "    # Print first chunk as sample\n",
    "    if chunks:\n",
    "        print(\"\\nSample chunk (first 200 characters):\")\n",
    "        print(chunks[0][:200] + \"...\")\n",
    "else:\n",
    "    print(\"No PDF documents found in the data directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatBot tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing functions from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the object to call LLMs from groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planet with the most moons in our solar system is Jupiter. As of now, Jupiter has a total of 92 confirmed moons, with many more smaller, irregular moons waiting to be confirmed. \n",
      "\n",
      "Some of the largest and most notable moons of Jupiter include:\n",
      "- Io\n",
      "- Europa\n",
      "- Ganymede\n",
      "- Callisto\n",
      "\n",
      "These four moons are known as the Galilean moons, as they were discovered by Galileo Galilei in 1610. They are some of the largest moons in the solar system and offer valuable insights into Jupiter's formation and evolution.\n",
      "\n",
      "Saturn is the second planet with the most moons, having a total of 83 confirmed moons. The other planets in our solar system have fewer moons, with Uranus having 27 and Neptune having 14.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what planet in the solar system has more moons?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pdf files and trasnforming to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "current_wd = os.getcwd()\n",
    "data_path = os.path.join(current_wd, 'data')\n",
    "raw_documents = os.listdir(data_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file_name in raw_documents:\n",
    "    with pdfplumber.open(os.path.join(data_path, file_name)) as pdf:\n",
    "        text = ''\n",
    "        for p in pdf.pages:\n",
    "            text += p.extract_text().replace('\\n', ' ')\n",
    "        documents.append({\"id\": file_name, \"text\": text})\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk the text from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n"
     ]
    }
   ],
   "source": [
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Load Pretrained Model and Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Example Text\n",
    "# text = \"Hello, how are you?\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# # Run the input through the model\n",
    "# with torch.inference_mode():  # Disable gradient calculation for efficiency\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Extract the hidden states (last layer embeddings)\n",
    "# last_hidden_states = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# cls_embedding = last_hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "# print(cls_embedding.shape)  # Output: torch.Size([1, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_assignment_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "    def __call__(self, input_texts):\n",
    "        if isinstance(input_texts, str):\n",
    "            input_texts = [input_texts]\n",
    "        embeddings = self.model.encode(input_texts, convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "custom_embeddings = MyEmbeddingFunction()\n",
    "\n",
    "\n",
    "chroma_client = PersistentClient(path=\"chroma_persistent_storage\")\n",
    "test_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"ghg_collection\",\n",
    "    embedding_function=custom_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks stored in ChromaDB: 2126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in chunked_documents:\n",
    "    test_collection.add(\n",
    "        ids=[chunk[\"id\"]],\n",
    "        embeddings=custom_embeddings([chunk[\"text\"]]),\n",
    "        metadatas=[{\"source\": chunk[\"id\"]}],\n",
    "        documents=[chunk[\"text\"]]\n",
    "    )\n",
    "\n",
    "print(f\"Total chunks stored in ChromaDB: {test_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Query Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(question, n_results=2):\n",
    "\n",
    "    query_embedding = custom_embeddings([question])\n",
    "\n",
    "    results = test_collection.query(query_embeddings=query_embedding, n_results=n_results)\n",
    "\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    print(\"==== Returning relevant chunks ====\")\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are a digital consultant specializing in Australia's evolving greenhouse gas (GHG) emission regulations. \"\n",
    "        \"Your task is to help companies navigate the complexities of compliance, accurate emission calculations, and industry-specific scope definitions. \"\n",
    "        \"Use the following context to provide tailored, concise, and accurate guidance. Ensure the response is practical, actionable, and aligned with the most recent regulatory updates. \"\n",
    "        \"If the answer is not available or unclear, state that you do not know. \"\n",
    "        \"Use five sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
    "    )\n",
    "\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Returning relevant chunks ====\n",
      "To calculate Scope 3 emissions, your company should follow the Australian Government's National Greenhouse and Energy Reporting (NGER) scheme and the GHG Protocol Corporate Standard. Scope 3 emissions include indirect emissions from sources not owned or controlled by your company, such as supply chain, transportation, and employee commuting. You can use the GHG Protocol's Scope 3 calculation guidance and tools to estimate these emissions. The Australian Government also provides resources and tools to support Scope 3 emissions calculation, including the NGER Scope 3 Emissions Estimation Tool. It is recommended to consult with a specialist or the relevant Australian authorities to ensure compliance with the latest regulatory requirements.\n"
     ]
    }
   ],
   "source": [
    "question = \"How should my company calculate emissions for Scope 3?\"\n",
    "relevant_chunks = query_documents(question)\n",
    "response = generate_response(question, relevant_chunks)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_assignment_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
