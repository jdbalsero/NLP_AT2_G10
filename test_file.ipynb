{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing functions from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=openai_key, model_name=\"text-embedding-3-small\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializa chroma client with persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagocardenas/Documents/MDSI/202501/applied nlp/NLP_AT2_G10/my_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "chroma_client = PersistentClient(path=\"chroma_persistent_storage\")\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name, embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the object to call LLMs from groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planet in our solar system with the most moons is Jupiter. As of my knowledge cutoff in 2023, Jupiter has a total of 92 confirmed moons. However, it's essential to note that this number might change over time as new discoveries are made or classifications are updated.\n",
      "\n",
      "Some of the most notable moons of Jupiter include:\n",
      "1. Ganymede: The largest moon in the solar system, even bigger than the planet Mercury.\n",
      "2. Io: The most volcanically active body in the solar system.\n",
      "3. Europa: A moon with a potential subsurface ocean, making it a fascinating target for astrobiological research.\n",
      "4. Callisto: The outermost of the four largest moons of Jupiter, known for its cratered surface.\n",
      "\n",
      "Saturn is the second planet with the most moons, with a total of 83 confirmed moons as of my knowledge cutoff in 2023. The other planets in our solar system have significantly fewer moons.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what planet in the solar system has more moons?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pdf files and trasnforming to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "current_wd = os.getcwd()\n",
    "data_path = os.path.join(current_wd, 'data')\n",
    "raw_documents = os.listdir(data_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file_name in raw_documents:\n",
    "    with pdfplumber.open(os.path.join(data_path, file_name)) as pdf:\n",
    "        text = ''\n",
    "        for p in pdf.pages:\n",
    "            text += p.extract_text().replace('\\n', ' ')\n",
    "        documents.append({\"id\": file_name, \"text\": text})\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk the text from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2126"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagocardenas/Documents/MDSI/202501/applied nlp/NLP_AT2_G10/my_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load Pretrained Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example Text\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Run the input through the model\n",
    "with torch.inference_mode():  # Disable gradient calculation for efficiency\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the hidden states (last layer embeddings)\n",
    "last_hidden_states = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "cls_embedding = last_hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "print(cls_embedding.shape)  # Output: torch.Size([1, 768])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
